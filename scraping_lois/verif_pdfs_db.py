import os
import polars as pl
import boto3
from dotenv import load_dotenv
from PyPDF2 import PdfReader
from botocore.exceptions import ClientError
from datetime import datetime
import io
import warnings
import tempfile
import shutil

warnings.filterwarnings('ignore', message='Unverified HTTPS request')

load_dotenv()

TEMP_DIR = tempfile.gettempdir()
LOG_VERIF_DIR = os.path.join(TEMP_DIR, "verif_db_logs")

os.makedirs(LOG_VERIF_DIR, exist_ok=True)

DB_TEMP_PATH = os.path.join(TEMP_DIR, "db_urls.parquet.tmp")
DB_FILENAME = "db_urls.parquet"

BUCKET_NAME = os.getenv("BUCKET_NAME")
ENDPOINT_URL = os.getenv("R2_ENDPOINT_URL")
ACCESS_KEY = os.getenv("R2_ACCESS_KEY_ID")
SECRET_KEY = os.getenv("R2_SECRET_ACCESS_KEY")

s3 = boto3.client(
    's3',
    endpoint_url=ENDPOINT_URL,
    aws_access_key_id=ACCESS_KEY,
    aws_secret_access_key=SECRET_KEY,
    verify=False
)

logfile = os.path.join(LOG_VERIF_DIR, f"pdf_verification_{datetime.now().strftime('%Y-%m-%d_%H-%M')}.log")

def log(message: str):
    timestamp = datetime.now().isoformat()
    line = f"{timestamp} ‚Äî {message}"
    print(line)
    with open(logfile, "a", encoding="utf-8") as f:
        f.write(line + "\n")
    return line

def verify_pdf_readability(pdf_stream, pdf_name):
    """
    V√©rifie si un PDF peut √™tre ouvert et lu
    Retourne: (is_readable: bool, error_message: str)
    """
    try:
        pdf_reader = PdfReader(pdf_stream)
        num_pages = len(pdf_reader.pages)
        
        if num_pages > 0:
            _ = pdf_reader.pages[0].extract_text()
        
        return True, None
    except Exception as e:
        return False, str(e)

def check_all_pdfs_on_cloud():
    """
    V√©rifie tous les PDFs sur Scaleway et met √† jour is_corrupted
    """
    log("\n" + "="*50)
    log("üîç V√âRIFICATION DES PDFs SUR SCALEWAY")
    log("="*50)
    
    corrupted_urls = []
    readable_count = 0
    total_checked = 0
    
    try:
        log("üì• T√©l√©chargement de la DB...")
        s3.download_file(BUCKET_NAME, DB_FILENAME, DB_TEMP_PATH)
        df = pl.read_parquet(DB_TEMP_PATH)
    
        
        cloud_keys = (
            df.filter(pl.col("downloaded") == True)
              .select([
                  pl.concat_str([pl.lit("pdfs/"), pl.col("pdf_name")]).alias("cloud_key"),
                  pl.col("pdf_name"),
                  pl.col("url")
              ])
        )
        
        total_pdfs = len(cloud_keys)
        log(f"üìä {total_pdfs} PDFs √† v√©rifier\n")
        
        for idx, row in enumerate(cloud_keys.iter_rows(named=True), 1):
            cloud_key = row["cloud_key"]
            pdf_name = row["pdf_name"]
            url = row["url"]
            
            try:
                pdf_obj = s3.get_object(Bucket=BUCKET_NAME, Key=cloud_key)
                pdf_stream = io.BytesIO(pdf_obj['Body'].read())
                
                is_readable, error_msg = verify_pdf_readability(pdf_stream, pdf_name)
                
                total_checked += 1
                
                if is_readable:
                    readable_count += 1
                    log(f"[{idx}/{total_pdfs}] ‚úÖ {pdf_name}")
                else:
                    corrupted_urls.append(url)
                    log(f"[{idx}/{total_pdfs}] ‚ùå {pdf_name} ‚Äî ERREUR: {error_msg}")
                
            except ClientError as e:
                log(f"[{idx}/{total_pdfs}] ‚ö†Ô∏è {pdf_name} ‚Äî ERREUR CLOUD: {e}")
                corrupted_urls.append(url)
            except Exception as e:
                log(f"[{idx}/{total_pdfs}] ‚ö†Ô∏è {pdf_name} ‚Äî ERREUR INCONNUE: {e}")
                corrupted_urls.append(url)
        
        log("\nüìù Mise √† jour de la DB...")
        df = df.with_columns(
            pl.when(pl.col("url").is_in(corrupted_urls))
              .then(True)
              .otherwise(pl.col("is_corrupted"))
              .alias("is_corrupted")
        )
        
        corrupted_count = len(corrupted_urls)
        
        log("\n" + "="*50)
        log("üìà R√âSUM√â DE LA V√âRIFICATION")
        log("="*50)
        log(f"Total v√©rifi√© : {total_checked}")
        log(f"‚úÖ Lisibles : {readable_count}")
        log(f"‚ùå Corrompus/Illisibles : {corrupted_count}")
        
        if corrupted_urls:
            log("\n" + "="*50)
            log("‚ö†Ô∏è LISTE DES PDFs CORROMPUS/ILLISIBLES:")
            log("="*50)
            
            corrupted_df = df.filter(pl.col("url").is_in(corrupted_urls))
            for row in corrupted_df.iter_rows(named=True):
                log(f"  - {row['pdf_name']}")
                log(f"    URL: {row['url']}")
        else:
            log("\nüéâ Tous les PDFs sont lisibles!")
        
        df.write_parquet(DB_TEMP_PATH)
        log("\nüíæ DB mise √† jour localement")
        
        log("‚òÅÔ∏è Upload de la DB vers Scaleway...")
        s3.upload_file(DB_TEMP_PATH, BUCKET_NAME, DB_FILENAME)
        log("‚úÖ DB synchronis√©e sur le Cloud")
        
        log("\n‚òÅÔ∏è Upload du log...")
        log_name = os.path.basename(logfile)
        s3.upload_file(logfile, BUCKET_NAME, f"pdfs-assemblee-nationale/logs/verif_db/{log_name}")
        log("‚úÖ Log upload√© sur Scaleway")
        
        log("\n" + "="*50)
        log("üèÅ V√âRIFICATION TERMIN√âE")
        log("="*50)
        
    except Exception as e:
        log(f"\n‚ùå ERREUR CRITIQUE: {e}")
        import traceback
        log(traceback.format_exc())
    
    finally:
        try:
            if os.path.exists(DB_TEMP_PATH):
                os.remove(DB_TEMP_PATH)
                print("üóëÔ∏è Fichier DB temporaire supprim√©")
            
            if os.path.exists(logfile):
                os.remove(logfile)
            
            if os.path.exists(LOG_VERIF_DIR):
                shutil.rmtree(LOG_VERIF_DIR)
            
            print("üóëÔ∏è Fichiers temporaires supprim√©s")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur nettoyage: {e}")

if __name__ == "__main__":
    check_all_pdfs_on_cloud()
